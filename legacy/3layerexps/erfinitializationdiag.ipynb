{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46d3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '/home/akiva/FCNX-Ensembling')\n",
    "\n",
    "from FCN3Network import FCN3NetworkEnsembleErf\n",
    "\n",
    "def custom_mse_loss(outputs, targets):\n",
    "    diff = outputs - targets\n",
    "    return torch.sum(diff * diff)\n",
    "    # Initialize FCN3NetworkEnsembleErf with ens=10, d=20, P=20, N=400\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8315b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FCN3NetworkEnsembleErf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m P = \u001b[32m10000\u001b[39m\n\u001b[32m      5\u001b[39m N = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mFCN3NetworkEnsembleErf\u001b[49m(d, N,N,P, ens=ens, weight_initialization_variance=(\u001b[32m1\u001b[39m/d,\u001b[32m1\u001b[39m/N, \u001b[32m1\u001b[39m/(N)), device=\u001b[33m'\u001b[39m\u001b[33mcuda:1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m torch.manual_seed(\u001b[32m613\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Create data on GPU directly and pin memory\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'FCN3NetworkEnsembleErf' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "ens = 100\n",
    "d = 20\n",
    "P = 10000\n",
    "N = 10\n",
    "model = FCN3NetworkEnsembleErf(d, N,N,P, ens=ens, weight_initialization_variance=(1/d,1/N, 1/(N)), device='cuda:1')\n",
    "torch.manual_seed(613)\n",
    "# Create data on GPU directly and pin memory\n",
    "X = torch.randn((P, d), device=device)\n",
    "Y = X[:, 0].unsqueeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ac367f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yr = Y.unsqueeze(-1)\n",
    "Yr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc5bda48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3074, device='cuda:1', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(X)\n",
    "loss = custom_mse_loss(outputs, Yr)\n",
    "avg_loss = loss / (ens * P)\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5fcc127c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b3d685c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4380, device='cuda:1', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((outputs-Y.unsqueeze(-1))**2).sum() / (10 * P * 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b56ddc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(175.1940, device='cuda:1', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((outputs-Y.unsqueeze(-2))**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3998580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20, 10])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs-Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "29a1bc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.8752, device='cuda:1', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((outputs-Y)**2).sum()/ (10 * P * 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "01545b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf13add0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1222, device='cuda:1', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model(X)**2).sum() / (P * 2 * 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d8e74b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0196, device='cuda:1')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb15e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68319c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 2 GPU(s) for utilization...\n",
      "  Device 0 (NVIDIA GeForce RTX 4090): 0.00 MB allocated\n",
      "  Device 1 (NVIDIA GeForce RTX 4080): 0.00 MB allocated\n",
      "\n",
      "Selected device: cuda:0 (Least allocated memory: 0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus == 0:\n",
    "    print(\"No CUDA devices found. Using CPU.\")\n",
    "\n",
    "least_memory_allocated = float('inf')\n",
    "least_utilized_device = None\n",
    "\n",
    "print(f\"Checking {num_gpus} GPU(s) for utilization...\")\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    device = torch.device(f\"cuda:{i}\")\n",
    "    # Set the current device to query its memory\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "    # Get allocated memory (bytes)\n",
    "    # memory_allocated() returns the total bytes that PyTorch has allocated on the GPU.\n",
    "    # memory_reserved() returns the total bytes that PyTorch has reserved (cached) on the GPU.\n",
    "    # allocated_bytes = torch.cuda.memory_allocated(device)\n",
    "    # reserved_bytes = torch.cuda.memory_reserved(device)\n",
    "    \n",
    "    # For simplicity, we'll use memory_allocated as a primary metric.\n",
    "    # You might consider memory_reserved or a combination for more nuanced decisions.\n",
    "    current_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    \n",
    "    print(f\"  Device {i} ({torch.cuda.get_device_name(i)}): {current_allocated_memory / (1024**2):.2f} MB allocated\")\n",
    "\n",
    "    if current_allocated_memory < least_memory_allocated:\n",
    "        least_memory_allocated = current_allocated_memory\n",
    "        least_utilized_device = device\n",
    "\n",
    "if least_utilized_device:\n",
    "    print(f\"\\nSelected device: {least_utilized_device} (Least allocated memory: {least_memory_allocated / (1024**2):.2f} MB)\")\n",
    "else:\n",
    "    # Fallback, should not happen if num_gpus > 0\n",
    "    print(\"Could not determine least utilized GPU. Falling back to CPU.\")\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe7465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
