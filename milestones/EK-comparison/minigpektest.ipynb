{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c8a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Running Scaling Experiment: d=10, kappa=1.0\n",
      "-----------------------------------------------------------------\n",
      "n      | P      | Bias Th   | Bias Emp  | Var Th    | Var Emp  \n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.linalg' has no attribute 'cholesky_solve'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    104\u001b[39m b_th, v_th = compute_theoretical_loss(d, n_val, kappa)\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Empirical\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m b_emp, v_emp = \u001b[43mrun_empirical_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mbias_th\u001b[39m\u001b[33m'\u001b[39m].append(b_th)\n\u001b[32m    110\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mvar_th\u001b[39m\u001b[33m'\u001b[39m].append(v_th)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mrun_empirical_simulation\u001b[39m\u001b[34m(d, n, kappa, n_trials, n_test)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m# Equivalent to scipy.linalg.cho_factor/solve\u001b[39;00m\n\u001b[32m     56\u001b[39m     L = torch.linalg.cholesky(reg_matrix)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     alpha = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky_solve\u001b[49m(y_train, L)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m torch.linalg.LinAlgError:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Fallback to general solver if not positive definite\u001b[39;00m\n\u001b[32m     60\u001b[39m     alpha = torch.linalg.solve(reg_matrix, y_train)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.linalg' has no attribute 'cholesky_solve'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. Definitions\n",
    "# ==========================================\n",
    "\n",
    "def compute_theoretical_loss(d, n, kappa):\n",
    "    \"\"\"Theoretical EK Loss based on user formula.\"\"\"\n",
    "    P = n * d\n",
    "    lH = 1.0 / d\n",
    "    denom = lH + (kappa / P)\n",
    "    \n",
    "    bias_theo = ((kappa / P) / denom) ** 2\n",
    "    var_theo = (kappa / P) * (lH / denom)\n",
    "    \n",
    "    return bias_theo, var_theo\n",
    "\n",
    "def kernel_fn(X1, X2, d):\n",
    "    \"\"\"Linear Kernel normalized by d.\"\"\"\n",
    "    return (X1 @ X2.T) / d\n",
    "\n",
    "def run_empirical_simulation(d, n, kappa, n_trials=30, n_test=1000):\n",
    "    \"\"\"Runs GPR simulation entirely on CUDA.\"\"\"\n",
    "    P = int(n * d)\n",
    "\n",
    "    # 1. Teacher / Ground Truth (Unit vector)\n",
    "    w_star = torch.randn(d, 1, device=device)\n",
    "    w_star = w_star / torch.norm(w_star)\n",
    "\n",
    "    # 2. Test Set\n",
    "    X_test = torch.randn(n_test, d, device=device)\n",
    "    f_test = X_test @ w_star  # (n_test, 1)\n",
    "\n",
    "    # Pre-allocate predictions matrix on GPU\n",
    "    predictions = torch.zeros((n_trials, n_test), device=device)\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # A. Train Data\n",
    "        X_train = torch.randn(P, d, device=device)\n",
    "        y_train = X_train @ w_star # (P, 1)\n",
    "\n",
    "        # B. Kernel Matrix\n",
    "        K_train = kernel_fn(X_train, X_train, d)\n",
    "\n",
    "        # C. Solve with SCALED Ridge using Torch Native Linalg\n",
    "        reg_matrix = K_train + kappa * torch.eye(P, device=device)\n",
    "\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(reg_matrix)\n",
    "            # Use torch.cholesky_solve instead of torch.linalg.cholesky_solve\n",
    "            alpha = torch.cholesky_solve(y_train, L)\n",
    "        except (torch.linalg.LinAlgError, RuntimeError):\n",
    "            # Fallback to general solver if not positive definite\n",
    "            alpha = torch.linalg.solve(reg_matrix, y_train)\n",
    "\n",
    "        # D. Predict\n",
    "        K_test = kernel_fn(X_test, X_train, d)\n",
    "        predictions[i, :] = (K_test @ alpha).squeeze()\n",
    "\n",
    "    # 3. Compute Metrics\n",
    "    f_bar = predictions.mean(dim=0)\n",
    "\n",
    "    # Bias: Error of the mean predictor\n",
    "    bias_emp = torch.mean((f_bar - f_test.squeeze())**2).item()\n",
    "\n",
    "    # Variance: Average variance of predictors around mean\n",
    "    var_emp = torch.mean(torch.var(predictions, dim=0)).item()\n",
    "\n",
    "    return bias_emp, var_emp\n",
    "\n",
    "# ==========================================\n",
    "# 2. Experiment Setup\n",
    "# ==========================================\n",
    "\n",
    "d = 10            \n",
    "kappa = 1.0        \n",
    "n_trials = 100      \n",
    "\n",
    "# Setup n_values directly on torch\n",
    "n_values = torch.logspace(-1, 2, 10).to(device)\n",
    "\n",
    "results = {\n",
    "    'n': n_values.cpu().numpy(),\n",
    "    'bias_th': [], 'var_th': [],\n",
    "    'bias_emp': [], 'var_emp': []\n",
    "}\n",
    "\n",
    "print(f\"Running Scaling Experiment: d={d}, kappa={kappa}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'n':<6} | {'P':<6} | {'Bias Th':<9} | {'Bias Emp':<9} | {'Var Th':<9} | {'Var Emp':<9}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for n in n_values:\n",
    "    n_val = n.item()\n",
    "    # Theory\n",
    "    b_th, v_th = compute_theoretical_loss(d, n_val, kappa)\n",
    "    \n",
    "    # Empirical\n",
    "    b_emp, v_emp = run_empirical_simulation(d, n_val, kappa, n_trials=n_trials)\n",
    "    \n",
    "    results['bias_th'].append(b_th)\n",
    "    results['var_th'].append(v_th)\n",
    "    results['bias_emp'].append(b_emp)\n",
    "    results['var_emp'].append(v_emp)\n",
    "    \n",
    "    print(f\"{n_val:<6.2f} | {int(n_val*d):<6} | {b_th:<9.4f} | {b_emp:<9.4f} | {v_th:<9.4f} | {v_emp:<9.4f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Plotting\n",
    "# ==========================================\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Bias Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.loglog(results['n'], results['bias_th'], 'k-', label='Theory')\n",
    "plt.loglog(results['n'], results['bias_emp'], 'ro', label='Empirical')\n",
    "plt.title('Bias')\n",
    "plt.xlabel('n (P/d)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Variance Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.loglog(results['n'], results['var_th'], 'k-', label='Theory')\n",
    "plt.loglog(results['n'], results['var_emp'], 'bo', label='Empirical')\n",
    "plt.title('Variance')\n",
    "plt.xlabel('n (P/d)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925db66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
